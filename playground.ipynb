{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datetime import datetime\n",
    "from preprocess import data_loader, df_to_dict\n",
    "from evaluation import evaluate\n",
    "from learner_functions import perform_active_learning\n",
    "from dataset_utils import genenrate_start_indices, dict_to_transformer_dataset\n",
    "\n",
    "from small_text import (\n",
    "    EmptyPoolException,\n",
    "    PoolBasedActiveLearner,\n",
    "    PoolExhaustedException,\n",
    "    RandomSampling,\n",
    "    random_initialization_balanced\n",
    ")\n",
    "from small_text.query_strategies.strategies import (QueryStrategy,\n",
    "                                                    RandomSampling,\n",
    "                                                    ConfidenceBasedQueryStrategy,\n",
    "                                                    LeastConfidence,\n",
    "                                                    EmbeddingBasedQueryStrategy,\n",
    "                                                    EmbeddingKMeans,\n",
    "                                                    ContrastiveActiveLearning)\n",
    "from small_text.integrations.transformers.classifiers.classification import TransformerModelArguments\n",
    "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args():\n",
    "    parser=argparse.ArgumentParser(description=\"Active Learning Experiment Runner with Transformers Integration\")\n",
    "    parser.add_argument('--method', type = str, metavar =\"\", default = 'AL', help=\"Supervised == SL or Active == AL\")\n",
    "    parser.add_argument('--framework', type = str, metavar =\"\", default = 'TF', help=\"Transformers == TF or SkLearn == SK\")\n",
    "    parser.add_argument('--datadir', type = str, metavar =\"\",default = './data/', help=\"Path to directory with data files\")\n",
    "    parser.add_argument('--dataset', type = str, metavar =\"\",default = 'wiki', help=\"Name of dataset\")\n",
    "    parser.add_argument('--outdir', type = str, metavar =\"\",default = './results/', help=\"Path to output directory for storing results\")\n",
    "    parser.add_argument('--transformer_model', type = str, metavar =\"\",default = 'distilroberta-base', help=\"Name of HuggingFace transformer model\")\n",
    "    parser.add_argument('--n_epochs', type = int, metavar =\"\",default =  5, help = \"Number of epochs for model training\")\n",
    "    parser.add_argument('--batch_size', type = int, metavar =\"\", default = 16, help = 'Number of samples per batch')\n",
    "    parser.add_argument('--eval_steps', type = int, metavar =\"\", default = 20000, help = 'Evaluation after a number of training steps')\n",
    "    parser.add_argument('--class_imbalance', type = int, metavar =\"\", default = 50, help = 'Class imbalance desired in train dataset')\n",
    "    parser.add_argument('--init_n', type = int, metavar =\"\", default = 20, help = 'Initial batch size for training')\n",
    "    parser.add_argument('--cold_strategy', metavar =\"\", default = 'BalancedRandom', help = 'Method of cold start to select initial examples')\n",
    "    parser.add_argument('--query_n', type = int, metavar =\"\", default = 100, help = 'Batch size per active learning query for training')\n",
    "    parser.add_argument('--query_strategy', metavar =\"\", default = 'LeastConfidence()', help = 'Method of active learning query for training')\n",
    "    parser.add_argument('--train_n', type = int, metavar =\"\", default = 20000, help = 'Total number of training examples')\n",
    "    parser.add_argument('--test_n', type = int, metavar =\"\", default = 5000, help = 'Total number of testing examples')\n",
    "    parser.add_argument('--run_n', type = int, metavar =\"\", default = 5, help = 'Number of times to run each model')\n",
    "    # parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args=parser.parse_args()\n",
    "    print(\"the inputs are:\")\n",
    "    for arg in vars(args):\n",
    "        print(\"{} is {}\".format(arg, getattr(args, arg)))\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the inputs are:\n",
      "method is AL\n",
      "framework is /home/raymond/.local/share/jupyter/runtime/kernel-v2-550fBo6Yk8T3J13.json\n",
      "datadir is ./data/\n",
      "dataset is wiki\n",
      "outdir is ./results/\n",
      "transformer_model is distilbert-base-uncased\n",
      "n_epochs is 5\n",
      "batch_size is 16\n",
      "eval_steps is 20000\n",
      "class_imbalance is 50\n",
      "init_n is 20\n",
      "cold_strategy is BalancedRandom\n",
      "query_n is 100\n",
      "query_strategy is LeastConfidence()\n",
      "train_n is 20000\n",
      "test_n is 5000\n",
      "run_n is 5\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now()\n",
    "args=parse_args()\n",
    "args.framework = 'TF'\n",
    "EXP_DIR = f'{args.outdir}/{args.method}_{args.framework}_{args.dataset}_{args.class_imbalance}_{args.train_n}'\n",
    "if not os.path.exists(EXP_DIR):\n",
    "    os.makedirs(EXP_DIR)\n",
    "output = {}\n",
    "for arg in vars(args):\n",
    "    output[arg] = getattr(args, arg)\n",
    "    \n",
    "logging.basicConfig(filename=f\"{EXP_DIR}/log.txt\",level=logging.DEBUG)\n",
    "logging.captureWarnings(True)\n",
    "logf = open(f\"{EXP_DIR}/err.log\", \"w\")\n",
    "with open(f'{EXP_DIR}/START_{current_datetime}.json', 'w') as fp:\n",
    "    json.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----RUN 0: AL LEARNER----\n",
      "----Seed: 0----\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "predictions_dict = {}\n",
    "# Load data\n",
    "train_df, test_dfs = data_loader(args)\n",
    "for run in range(1):\n",
    "    seed_value = run\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    print(f'----RUN {run}: {args.method} LEARNER----')\n",
    "    print(f'----Seed: {seed_value}----')\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.transformer_model, cache_dir='.cache/')    \n",
    "    tokenizer.add_special_tokens({'additional_special_tokens': [\"[URL]\", \"[EMOJI]\", \"[USER]\"]})\n",
    "    \n",
    "    test_datasets = {}\n",
    "    matching_indexes = {}\n",
    "    for j in test_dfs.keys():\n",
    "        matching_indexes[j] = test_dfs[j].index.tolist()\n",
    "        data_dict = df_to_dict('test', test_dfs[j])\n",
    "        processed_data = dict_to_transformer_dataset(data_dict, tokenizer)\n",
    "        test_datasets[j] = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y selected [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Starting imbalance (train): 0.5\n",
      "Starting imbalance: 0.5\n",
      "Setting val indices\n"
     ]
    }
   ],
   "source": [
    "train_dict = df_to_dict('train', train_df)\n",
    "indices_initial, y_initial, val_indices = genenrate_start_indices(train_dict, args)\n",
    "\n",
    "train_trans_dataset = dict_to_transformer_dataset(train_dict, tokenizer)\n",
    "\n",
    "transformer_model = TransformerModelArguments(args.transformer_model)\n",
    "clf_factory = TransformerBasedClassificationFactory(transformer_model,\n",
    "                                                    num_classes=2,\n",
    "                                                    kwargs={\n",
    "                                                        'device': 'cuda', \n",
    "                                                        'num_epochs': args.n_epochs,\n",
    "                                                        'mini_batch_size': args.batch_size,\n",
    "                                                        'class_weight': 'balanced'\n",
    "                                                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TransformerBasedClassificationFactory' object has no attribute 'classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m clf_factory\u001b[38;5;241m.\u001b[39mclassifier\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerBasedClassificationFactory' object has no attribute 'classifier'"
     ]
    }
   ],
   "source": [
    "transformer_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[URL]', '[USER]', '[EMOJI]']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30522: AddedToken(\"[URL]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30523: AddedToken(\"[USER]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t30524: AddedToken(\"[EMOJI]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_labeled = active_learner.initialize_data(indices_initial, y_initial, indices_validation=val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----Initalising----\n",
      "\n",
      "Learner initalized ok.\n"
     ]
    }
   ],
   "source": [
    "query_strategy = eval(args.query_strategy)\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train_trans_dataset)\n",
    "\n",
    "print('\\n----Initalising----\\n')\n",
    "iter_results_dict = {}\n",
    "iter_preds_dict = {}\n",
    "indices_labeled = active_learner.initialize_data(indices_initial, y_initial, indices_validation=val_indices)\n",
    "print('Learner initalized ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<small_text.integrations.transformers.classifiers.classification.TransformerBasedClassification at 0x7f8089ed1bd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "active_learner.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[  101,  1036,  8840,  2140,  1010,  2145,  9345,  4667,  2026,  3160,\n",
       "            1029,  2073,  2003,  1996,  3120,  2008,  1036,  1036, 14163, 20051,\n",
       "            3406,  1036,  1036,  2003,  1037,  1036,  1036, 14286, 11701,  1036,\n",
       "            1036,  2744,  1029,  1996,  2783,  1036,  1036,  3120,  1036,  1036,\n",
       "            7604,  2178,  3120,  2029,  2987,  1005,  1056,  2360,  2012,  2035,\n",
       "            2054,  1996,  2034,  3120,  1036,  1036, 14964,  1036,  1036,  1012,\n",
       "            1036,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0]]),\n",
       "  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       "  0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trans_dataset[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raymond/Library/Jupyter/runtime/kernel-v2-4635c68Se6l7zTgJ.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_dict['data'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = TextDataset(train_encodings, train_dict['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n"
     ]
    }
   ],
   "source": [
    "val_indices = _genenrate_val_indices(train_dict['target'])\n",
    "indices = np.arange(len(train_dict['target']))\n",
    "val_mask = np.isin(indices, val_indices)\n",
    "train_indices = indices[~val_mask]\n",
    "\n",
    "train_dataset = Subset(train_full, train_indices)\n",
    "val_dataset = Subset(train_full, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n",
      "(20000,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "indices_neg_label = np.where(train_full.y == 0)[0]\n",
    "indices_pos_label = np.where(train_full.y == 1)[0]\n",
    "all_indices = np.concatenate([indices_neg_label, indices_pos_label])\n",
    "np.random.shuffle(all_indices)\n",
    "x_indices_initial = all_indices.astype(int)\n",
    "y_initial = np.array([train_full.y[i] for i in x_indices_initial])\n",
    "print(f'Starting imbalance: {np.round(np.mean(y_initial),2)}')\n",
    "print('Setting val indices')\n",
    "val_indices = np.concatenate([np.random.choice(indices_pos_label, \n",
    "                                                int(0.1*len(indices_pos_label)),\n",
    "                                                replace=False),\n",
    "                                np.random.choice(indices_neg_label,\n",
    "                                                int(0.1*len(indices_neg_label)),\n",
    "                                                replace=False)\n",
    "                                ])\n",
    "indices = np.arange(x_indices_initial.shape[0])\n",
    "print(indices.shape)\n",
    "mask = np.isin(indices, val_indices)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_full[indices[~mask]]\n",
    "valid = train_full[indices[mask]]\n",
    "train_dataset = TensorDataset(torch.concat(train.x, dim=0), torch.Tensor(train.y))\n",
    "valid_dataset = TensorDataset(torch.concat(valid.x, dim=0), torch.Tensor(valid.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# disables the progress bar for notebooks: https://github.com/huggingface/datasets/issues/2651\n",
    "datasets.logging.get_verbosity = lambda: logging.NOTSET\n",
    "\n",
    "# set matplotlib params\n",
    "rcParams.update({'xtick.labelsize': 14, 'ytick.labelsize': 14, 'axes.labelsize': 16})\n",
    "\n",
    "# fix the random seed\n",
    "seed = 2022\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "\n",
    "raw_dataset = datasets.load_dataset('rotten_tomatoes')\n",
    "num_classes = raw_dataset['train'].features['label'].num_classes\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")\n",
    "\n",
    "\n",
    "target_labels = np.arange(num_classes)\n",
    "\n",
    "train = TransformersDataset.from_arrays(raw_dataset['train']['text'],\n",
    "                                        raw_dataset['train']['label'],\n",
    "                                        tokenizer,\n",
    "                                        max_length=60,\n",
    "                                        target_labels=target_labels)\n",
    "test = TransformersDataset.from_arrays(raw_dataset['test']['text'], \n",
    "                                       raw_dataset['test']['label'],\n",
    "                                       tokenizer,\n",
    "                                       max_length=60,\n",
    "                                       target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2178, 3793, 7099, 1012,  102,    0,    0]], device='mps:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 0, 0]], device='mps:0'),\n",
       " tensor([1], device='mps:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
