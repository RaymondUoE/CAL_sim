{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text import (\n",
    "    EmptyPoolException,\n",
    "    PoolBasedActiveLearner,\n",
    "    PoolExhaustedException,\n",
    "    RandomSampling,\n",
    "    random_initialization_balanced\n",
    ")\n",
    "from small_text.query_strategies.strategies import (QueryStrategy,\n",
    "                                                    RandomSampling,\n",
    "                                                    ConfidenceBasedQueryStrategy,\n",
    "                                                    LeastConfidence,\n",
    "                                                    EmbeddingBasedQueryStrategy,\n",
    "                                                    EmbeddingKMeans)\n",
    "from learner_functions import run_multiple_experiments\n",
    "from preprocess import (data_loader,\n",
    "                        df_to_dict)\n",
    "from SL_transformers_workaround import genenrate_val_indices\n",
    "from small_text.integrations.transformers.classifiers.classification import TransformerModelArguments\n",
    "from small_text.integrations.transformers.classifiers.factories import TransformerBasedClassificationFactory\n",
    "from small_text.integrations.transformers.datasets import TransformersDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6144/512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _genenrate_start_indices(train_dict, args):\n",
    "    if args.cold_strategy =='TrueRandom':\n",
    "        indices_neg_label = np.where(train_dict['target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['target'] == 1)[0]\n",
    "        all_indices = np.concatenate([indices_neg_label, indices_pos_label])\n",
    "        x_indices_initial = np.random.choice(all_indices,\n",
    "                                            args.init_n,\n",
    "                                            replace=False)\n",
    "    # Balanced Random Choice Based on Known Class label\n",
    "    elif args.cold_strategy == 'BalancedRandom': \n",
    "        indices_neg_label = np.where(train_dict['target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['target'] == 1)[0]\n",
    "        selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                int(args.init_n/2),\n",
    "                                                replace=False)\n",
    "        selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                int(args.init_n/2),\n",
    "                                                replace=False)\n",
    "        x_indices_initial = np.concatenate([selected_neg_label, selected_pos_label])\n",
    "    # Balanced Random Choice Based on Keywords (Weak label)\n",
    "    elif args.cold_strategy == 'BalancedWeak': \n",
    "        indices_neg_label = np.where(train_dict['weak_target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['weak_target'] == 1)[0]\n",
    "        if len(indices_pos_label) > int(args.init_n/2):\n",
    "            selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                    int(args.init_n/2),\n",
    "                                                    replace=False)\n",
    "            selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                    int(args.init_n/2),\n",
    "                                                    replace=False)\n",
    "        # If limit reached, take as many positive as possible and pad with negatives\n",
    "        else:\n",
    "            selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                    len(indices_pos_label),\n",
    "                                                    replace=False)\n",
    "            selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                    int(args.init_n) - len(indices_pos_label),\n",
    "                                                    replace=False)\n",
    "        x_indices_initial = np.concatenate([selected_neg_label, selected_pos_label])\n",
    "    else:\n",
    "        print('Invalid Cold Start Policy')\n",
    "    # Set x and y initial\n",
    "    x_indices_initial = x_indices_initial.astype(int)\n",
    "    y_initial = np.array([train_dict['target'][i] for i in x_indices_initial])\n",
    "    print('y selected', train_dict['target'][x_indices_initial])\n",
    "    print(f'Starting imbalance (train): {np.round(np.mean(y_initial),4)}')\n",
    "    # Set validation indices for transformers framework\n",
    "    val_indices = genenrate_val_indices(y_initial)\n",
    "    \n",
    "    return x_indices_initial, y_initial, val_indices\n",
    "\n",
    "def dict_to_transformer_dataset(data_dict, tokenizer):\n",
    "    encodings = tokenizer(data_dict['data'], truncation=True, padding=True)\n",
    "    return TransformersDataset(\n",
    "        [(torch.tensor(input_ids).reshape(1, -1), torch.tensor(attention_mask).reshape(1, -1), labels) \n",
    "         for input_ids, attention_mask, labels in \n",
    "              zip(encodings['input_ids'], encodings['attention_mask'], data_dict['target'])\n",
    "              ]\n",
    "        )\n",
    "    \n",
    "\n",
    "def parse_args():\n",
    "    parser=argparse.ArgumentParser(description=\"Active Learning Experiment Runner with Transformers Integration\")\n",
    "    parser.add_argument('--method', type = str, metavar =\"\", default = 'AL', help=\"Supervised == SL or Active == AL\")\n",
    "    parser.add_argument('--framework', type = str, metavar =\"\", default = 'TF', help=\"Transformers == TF or SkLearn == SK\")\n",
    "    parser.add_argument('--datadir', type = str, metavar =\"\",default = './data/', help=\"Path to directory with data files\")\n",
    "    parser.add_argument('--dataset', type = str, metavar =\"\",default = 'wiki', help=\"Name of dataset\")\n",
    "    parser.add_argument('--outdir', type = str, metavar =\"\",default = './results/', help=\"Path to output directory for storing results\")\n",
    "    parser.add_argument('--transformer_model', type = str, metavar =\"\",default = 'distilbert-base-uncased', help=\"Name of HuggingFace transformer model\")\n",
    "    parser.add_argument('--n_epochs', type = int, metavar =\"\",default =  5, help = \"Number of epochs for model training\")\n",
    "    parser.add_argument('--batch_size', type = int, metavar =\"\", default = 16, help = 'Number of samples per batch')\n",
    "    parser.add_argument('--eval_steps', type = int, metavar =\"\", default = 20000, help = 'Evaluation after a number of training steps')\n",
    "    parser.add_argument('--class_imbalance', type = int, metavar =\"\", default = 50, help = 'Class imbalance desired in train dataset')\n",
    "    parser.add_argument('--init_n', type = int, metavar =\"\", default = 20, help = 'Initial batch size for training')\n",
    "    parser.add_argument('--cold_strategy', metavar =\"\", default = 'BalancedWeak', help = 'Method of cold start to select initial examples')\n",
    "    parser.add_argument('--query_n', type = int, metavar =\"\", default = 100, help = 'Batch size per active learning query for training')\n",
    "    parser.add_argument('--query_strategy', metavar =\"\", default = 'LeastConfidence()', help = 'Method of active learning query for training')\n",
    "    parser.add_argument('--train_n', type = int, metavar =\"\", default = 20000, help = 'Total number of training examples')\n",
    "    parser.add_argument('--test_n', type = int, metavar =\"\", default = 5000, help = 'Total number of testing examples')\n",
    "    parser.add_argument('--run_n', type = int, metavar =\"\", default = 5, help = 'Number of times to run each model')\n",
    "    args=parser.parse_args()\n",
    "    print(\"the inputs are:\")\n",
    "    for arg in vars(args):\n",
    "        print(\"{} is {}\".format(arg, getattr(args, arg)))\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the inputs are:\n",
      "method is AL\n",
      "framework is /Users/raymond/Library/Jupyter/runtime/kernel-v2-4635LG7dVw9xiuV7.json\n",
      "datadir is ./data/\n",
      "dataset is wiki\n",
      "outdir is ./results/\n",
      "transformer_model is distilbert-base-uncased\n",
      "n_epochs is 5\n",
      "batch_size is 16\n",
      "eval_steps is 20000\n",
      "class_imbalance is 50\n",
      "init_n is 20\n",
      "cold_strategy is BalancedWeak\n",
      "query_n is 100\n",
      "query_strategy is LeastConfidence()\n",
      "train_n is 20000\n",
      "test_n is 5000\n",
      "run_n is 5\n",
      "y selected [1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "Starting imbalance (train): 0.7\n",
      "Starting imbalance: 0.7\n",
      "Setting val indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/data/datasets.py:36: UserWarning: Passing target_labels=None is discouraged as it can lead to unintended results in combination with indexing and cloning. Moreover, explicit target labels might be required in the next major version.\n",
      "  warnings.warn('Passing target_labels=None is discouraged as it can lead to '\n"
     ]
    }
   ],
   "source": [
    "args=parse_args()\n",
    "args.framework = 'TF'\n",
    "# Load data\n",
    "train_df, test_dfs = data_loader(args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.transformer_model, cache_dir='.cache/')    \n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"[URL]\", \"[EMOJI]\", \"[USER]\"]})\n",
    "train_dict = df_to_dict('train', train_df)\n",
    "indices_initial, y_initial, val_indices = _genenrate_start_indices(train_dict, args)\n",
    "\n",
    "train_trans_dataset = dict_to_transformer_dataset(train_dict, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = TransformerModelArguments(args.transformer_model)\n",
    "clf_factory = TransformerBasedClassificationFactory(transformer_model,\n",
    "                                                    num_classes=2,\n",
    "                                                    kwargs={\n",
    "                                                        'device': 'mps', \n",
    "                                                        'num_epochs': args.n_epochs,\n",
    "                                                        # 'mini_batch_size': 16,\n",
    "                                                        'class_weight': 'balanced'\n",
    "                                                    })\n",
    "query_strategy = eval(args.query_strategy)\n",
    "active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train_trans_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "<class 'small_text.integrations.transformers.datasets.TransformersDatasetView'>\n",
      "<class 'small_text.integrations.transformers.datasets.TransformersDatasetView'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indices_labeled \u001b[38;5;241m=\u001b[39m active_learner\u001b[38;5;241m.\u001b[39minitialize_data(indices_initial, y_initial, indices_validation\u001b[38;5;241m=\u001b[39mval_indices)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/active_learner.py:153\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner.initialize_data\u001b[0;34m(self, indices_initial, y_initial, indices_ignored, indices_validation, retrain)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices_ignored \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retrain:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrain(indices_validation\u001b[38;5;241m=\u001b[39mindices_validation)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/active_learner.py:406\u001b[0m, in \u001b[0;36mPoolBasedActiveLearner._retrain\u001b[0;34m(self, indices_validation)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(train))\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(valid))\n\u001b[0;32m--> 406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clf\u001b[38;5;241m.\u001b[39mfit(train, validation_set\u001b[38;5;241m=\u001b[39mvalid)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:378\u001b[0m, in \u001b[0;36mTransformerBasedClassification.fit\u001b[0;34m(self, train_set, validation_set, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialize_class_weights(sub_train)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_default_criterion(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weights_,\n\u001b[1;32m    376\u001b[0m                                              use_sample_weights\u001b[38;5;241m=\u001b[39mweights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_main(sub_train, sub_valid, sub_train_weights, early_stopping,\n\u001b[1;32m    379\u001b[0m                       model_selection, fit_optimizer, fit_scheduler)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/integrations/transformers/classifiers/classification.py:403\u001b[0m, in \u001b[0;36mTransformerBasedClassification._fit_main\u001b[0;34m(self, sub_train, sub_valid, weights, early_stopping, model_selection, optimizer, scheduler)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory(\u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39mget_tmp_dir_base()) \u001b[38;5;28;01mas\u001b[39;00m tmp_dir:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(sub_train, sub_valid, weights, early_stopping, model_selection,\n\u001b[1;32m    402\u001b[0m                 optimizer, scheduler, tmp_dir)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_perform_model_selection(optimizer, model_selection)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/integrations/pytorch/classifiers/base.py:57\u001b[0m, in \u001b[0;36mPytorchModelSelectionMixin._perform_model_selection\u001b[0;34m(self, optimizer, model_selection)\u001b[0m\n\u001b[1;32m     55\u001b[0m model_selection_result \u001b[38;5;241m=\u001b[39m model_selection\u001b[38;5;241m.\u001b[39mselect()\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_selection_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_selection_result\u001b[38;5;241m.\u001b[39mmodel_path))\n\u001b[1;32m     58\u001b[0m     optimizer_path \u001b[38;5;241m=\u001b[39m model_selection_result\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mwith_suffix(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt.optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(optimizer_path))\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[1;32m   1015\u001b[0m                      map_location,\n\u001b[1;32m   1016\u001b[0m                      pickle_module,\n\u001b[1;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[1;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/site-packages/torch/serialization.py:1357\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[1;32m   1356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1357\u001b[0m     storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "indices_labeled = active_learner.initialize_data(indices_initial, y_initial, indices_validation=val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<small_text.integrations.transformers.datasets.TransformersDatasetView at 0x2dae01690>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trans_dataset[indices_initial]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  1036,  8840,  2140,  1010,  2145,  9345,  4667,  2026,  3160,\n",
       "           1029,  2073,  2003,  1996,  3120,  2008,  1036,  1036, 14163, 20051,\n",
       "           3406,  1036,  1036,  2003,  1037,  1036,  1036, 14286, 11701,  1036,\n",
       "           1036,  2744,  1029,  1996,  2783,  1036,  1036,  3120,  1036,  1036,\n",
       "           7604,  2178,  3120,  2029,  2987,  1005,  1056,  2360,  2012,  2035,\n",
       "           2054,  1996,  2034,  3120,  1036,  1036, 14964,  1036,  1036,  1012,\n",
       "           1036,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trans_dataset.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 101, 7592, 2017,  102,    0,    0]),\n",
       " tensor([ 101, 3835, 2000, 3113, 2017,  102])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer(['hello you', 'nice to meet you'], padding=True)['input_ids']\n",
    "[torch.tensor(b) for b in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raymond/Library/Jupyter/runtime/kernel-v2-4635c68Se6l7zTgJ.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_dict['data'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = TextDataset(train_encodings, train_dict['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n"
     ]
    }
   ],
   "source": [
    "val_indices = _genenrate_val_indices(train_dict['target'])\n",
    "indices = np.arange(len(train_dict['target']))\n",
    "val_mask = np.isin(indices, val_indices)\n",
    "train_indices = indices[~val_mask]\n",
    "\n",
    "train_dataset = Subset(train_full, train_indices)\n",
    "val_dataset = Subset(train_full, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n",
      "(20000,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "indices_neg_label = np.where(train_full.y == 0)[0]\n",
    "indices_pos_label = np.where(train_full.y == 1)[0]\n",
    "all_indices = np.concatenate([indices_neg_label, indices_pos_label])\n",
    "np.random.shuffle(all_indices)\n",
    "x_indices_initial = all_indices.astype(int)\n",
    "y_initial = np.array([train_full.y[i] for i in x_indices_initial])\n",
    "print(f'Starting imbalance: {np.round(np.mean(y_initial),2)}')\n",
    "print('Setting val indices')\n",
    "val_indices = np.concatenate([np.random.choice(indices_pos_label, \n",
    "                                                int(0.1*len(indices_pos_label)),\n",
    "                                                replace=False),\n",
    "                                np.random.choice(indices_neg_label,\n",
    "                                                int(0.1*len(indices_neg_label)),\n",
    "                                                replace=False)\n",
    "                                ])\n",
    "indices = np.arange(x_indices_initial.shape[0])\n",
    "print(indices.shape)\n",
    "mask = np.isin(indices, val_indices)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_full[indices[~mask]]\n",
    "valid = train_full[indices[mask]]\n",
    "train_dataset = TensorDataset(torch.concat(train.x, dim=0), torch.Tensor(train.y))\n",
    "valid_dataset = TensorDataset(torch.concat(valid.x, dim=0), torch.Tensor(valid.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# disables the progress bar for notebooks: https://github.com/huggingface/datasets/issues/2651\n",
    "datasets.logging.get_verbosity = lambda: logging.NOTSET\n",
    "\n",
    "# set matplotlib params\n",
    "rcParams.update({'xtick.labelsize': 14, 'ytick.labelsize': 14, 'axes.labelsize': 16})\n",
    "\n",
    "# fix the random seed\n",
    "seed = 2022\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "\n",
    "raw_dataset = datasets.load_dataset('rotten_tomatoes')\n",
    "num_classes = raw_dataset['train'].features['label'].num_classes\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")\n",
    "\n",
    "\n",
    "target_labels = np.arange(num_classes)\n",
    "\n",
    "train = TransformersDataset.from_arrays(raw_dataset['train']['text'],\n",
    "                                        raw_dataset['train']['label'],\n",
    "                                        tokenizer,\n",
    "                                        max_length=60,\n",
    "                                        target_labels=target_labels)\n",
    "test = TransformersDataset.from_arrays(raw_dataset['test']['text'], \n",
    "                                       raw_dataset['test']['label'],\n",
    "                                       tokenizer,\n",
    "                                       max_length=60,\n",
    "                                       target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2178, 3793, 7099, 1012,  102,    0,    0]], device='mps:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 0, 0]], device='mps:0'),\n",
       " tensor([1], device='mps:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
