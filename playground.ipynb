{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text import (\n",
    "    EmptyPoolException,\n",
    "    PoolBasedActiveLearner,\n",
    "    PoolExhaustedException,\n",
    "    RandomSampling,\n",
    "    # TransformerBasedClassificationFactory,\n",
    "    # TransformerModelArguments,\n",
    "    random_initialization_balanced\n",
    ")\n",
    "from learner_functions import run_multiple_experiments\n",
    "from preprocess import (data_loader,\n",
    "                        preprocess_data_sklearn_train,\n",
    "                        preprocess_data_sklearn_test,\n",
    "                        preprocess_data_transformers,\n",
    "                        df_to_dict)\n",
    "from SL_transformers_workaround import genenrate_val_indices\n",
    "from small_text.integrations.transformers.classifiers.classification import TransformerModelArguments\n",
    "from small_text.integrations.transformers.classfiers.factories import TransformerBasedClassificationFactory\n",
    "\n",
    "# from preprocess import data_loader, preprocess_data_transformers, df_to_dict\n",
    "\n",
    "# from examplecode.data.corpus_twenty_news import get_twenty_newsgroups_corpus\n",
    "# from examplecode.data.example_data_transformers import preprocess_data\n",
    "# from examplecode.shared import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _genenrate_start_indices(train_dict, args):\n",
    "    if args.cold_strategy =='TrueRandom':\n",
    "        indices_neg_label = np.where(train_dict['target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['target'] == 1)[0]\n",
    "        all_indices = np.concatenate([indices_neg_label, indices_pos_label])\n",
    "        x_indices_initial = np.random.choice(all_indices,\n",
    "                                            args.init_n,\n",
    "                                            replace=False)\n",
    "    # Balanced Random Choice Based on Known Class label\n",
    "    elif args.cold_strategy == 'BalancedRandom': \n",
    "        indices_neg_label = np.where(train_dict['target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['target'] == 1)[0]\n",
    "        selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                int(args.init_n/2),\n",
    "                                                replace=False)\n",
    "        selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                int(args.init_n/2),\n",
    "                                                replace=False)\n",
    "        x_indices_initial = np.concatenate([selected_neg_label, selected_pos_label])\n",
    "    # Balanced Random Choice Based on Keywords (Weak label)\n",
    "    elif args.cold_strategy == 'BalancedWeak': \n",
    "        indices_neg_label = np.where(train_dict['weak_target'] == 0)[0]\n",
    "        indices_pos_label = np.where(train_dict['weak_target'] == 1)[0]\n",
    "        if len(indices_pos_label) > int(args.init_n/2):\n",
    "            selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                    int(args.init_n/2),\n",
    "                                                    replace=False)\n",
    "            selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                    int(args.init_n/2),\n",
    "                                                    replace=False)\n",
    "        # If limit reached, take as many positive as possible and pad with negatives\n",
    "        else:\n",
    "            selected_pos_label = np.random.choice(indices_pos_label,\n",
    "                                                    len(indices_pos_label),\n",
    "                                                    replace=False)\n",
    "            selected_neg_label = np.random.choice(indices_neg_label,\n",
    "                                                    int(args.init_n) - len(indices_pos_label),\n",
    "                                                    replace=False)\n",
    "        x_indices_initial = np.concatenate([selected_neg_label, selected_pos_label])\n",
    "    else:\n",
    "        print('Invalid Cold Start Policy')\n",
    "    # Set x and y initial\n",
    "    x_indices_initial = x_indices_initial.astype(int)\n",
    "    y_initial = np.array([train_dict['target'][i] for i in x_indices_initial])\n",
    "    print('y selected', train_dict['target'][x_indices_initial])\n",
    "    print(f'Starting imbalance (train): {np.round(np.mean(y_initial),4)}')\n",
    "    # Set validation indices for transformers framework\n",
    "    val_indices = genenrate_val_indices(y_initial)\n",
    "    \n",
    "    return x_indices_initial, y_initial, val_indices\n",
    "\n",
    "def parse_args():\n",
    "    parser=argparse.ArgumentParser(description=\"Active Learning Experiment Runner with Transformers Integration\")\n",
    "    parser.add_argument('--method', type = str, metavar =\"\", default = 'AL', help=\"Supervised == SL or Active == AL\")\n",
    "    parser.add_argument('--framework', type = str, metavar =\"\", default = 'TF', help=\"Transformers == TF or SkLearn == SK\")\n",
    "    parser.add_argument('--datadir', type = str, metavar =\"\",default = './data/', help=\"Path to directory with data files\")\n",
    "    parser.add_argument('--dataset', type = str, metavar =\"\",default = 'wiki', help=\"Name of dataset\")\n",
    "    parser.add_argument('--outdir', type = str, metavar =\"\",default = './results/', help=\"Path to output directory for storing results\")\n",
    "    parser.add_argument('--transformer_model', type = str, metavar =\"\",default = 'distilbert-base-uncased', help=\"Name of HuggingFace transformer model\")\n",
    "    parser.add_argument('--n_epochs', type = int, metavar =\"\",default =  5, help = \"Number of epochs for model training\")\n",
    "    parser.add_argument('--batch_size', type = int, metavar =\"\", default = 16, help = 'Number of samples per batch')\n",
    "    parser.add_argument('--eval_steps', type = int, metavar =\"\", default = 20000, help = 'Evaluation after a number of training steps')\n",
    "    parser.add_argument('--class_imbalance', type = int, metavar =\"\", default = 50, help = 'Class imbalance desired in train dataset')\n",
    "    parser.add_argument('--init_n', type = int, metavar =\"\", default = 20, help = 'Initial batch size for training')\n",
    "    parser.add_argument('--cold_strategy', metavar =\"\", default = 'BalancedWeak', help = 'Method of cold start to select initial examples')\n",
    "    parser.add_argument('--query_n', type = int, metavar =\"\", default = 100, help = 'Batch size per active learning query for training')\n",
    "    parser.add_argument('--query_strategy', metavar =\"\", default = 'LeastConfidence()', help = 'Method of active learning query for training')\n",
    "    parser.add_argument('--train_n', type = int, metavar =\"\", default = 20000, help = 'Total number of training examples')\n",
    "    parser.add_argument('--test_n', type = int, metavar =\"\", default = 5000, help = 'Total number of testing examples')\n",
    "    parser.add_argument('--run_n', type = int, metavar =\"\", default = 5, help = 'Number of times to run each model')\n",
    "    args=parser.parse_args()\n",
    "    print(\"the inputs are:\")\n",
    "    for arg in vars(args):\n",
    "        print(\"{} is {}\".format(arg, getattr(args, arg)))\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the inputs are:\n",
      "method is AL\n",
      "framework is /Users/raymond/Library/Jupyter/runtime/kernel-v2-4635LG7dVw9xiuV7.json\n",
      "datadir is ./data/\n",
      "dataset is wiki\n",
      "outdir is ./results/\n",
      "transformer_model is distilbert-base-uncased\n",
      "n_epochs is 5\n",
      "batch_size is 16\n",
      "eval_steps is 20000\n",
      "class_imbalance is 50\n",
      "init_n is 20\n",
      "cold_strategy is BalancedWeak\n",
      "query_n is 100\n",
      "query_strategy is LeastConfidence()\n",
      "train_n is 20000\n",
      "test_n is 5000\n",
      "run_n is 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/data/datasets.py:36: UserWarning: Passing target_labels=None is discouraged as it can lead to unintended results in combination with indexing and cloning. Moreover, explicit target labels might be required in the next major version.\n",
      "  warnings.warn('Passing target_labels=None is discouraged as it can lead to '\n"
     ]
    }
   ],
   "source": [
    "args=parse_args()\n",
    "args.framework = 'TF'\n",
    "# Load data\n",
    "train_df, test_dfs = data_loader(args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.transformer_model, cache_dir='.cache/')    \n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"[URL]\", \"[EMOJI]\", \"[USER]\"]})\n",
    "train_dict = df_to_dict('train', train_df)\n",
    "indices_initial, y_initial, val_indices = _genenrate_start_indices(train_dict)\n",
    "transformer_model = TransformerModelArguments(args.transformer_model)\n",
    "clf_factory = TransformerBasedClassificationFactory(transformer_model, \n",
    "                                                    num_classes, \n",
    "                                                    kwargs={\n",
    "                                                        'device': 'cuda', \n",
    "                                                        'mini_batch_size': 32,\n",
    "                                                        'class_weight': 'balanced'\n",
    "                                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'weak_target'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL = TransformerModelArguments(args.transformer_model)\n",
    "clf_factory = TransformerBasedClassificationFactory(TRANSFORMER_MODEL,\n",
    "                                                # 2,\n",
    "                                                args.n_epochs,\n",
    "                                                kwargs = {'device': 'mps'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/data/datasets.py:36: UserWarning: Passing target_labels=None is discouraged as it can lead to unintended results in combination with indexing and cloning. Moreover, explicit target labels might be required in the next major version.\n",
      "  warnings.warn('Passing target_labels=None is discouraged as it can lead to '\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/raymond/Projects/playground/CAL_sim/results/AL_/Users/raymond/Library/Jupyter/runtime/kernel-v2-4635c68Se6l7zTgJ.json_wiki_50_20_BalancedWeak_LeastConfidence()_100/log.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     test_sets[j] \u001b[38;5;241m=\u001b[39m processed_data\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m run_multiple_experiments(clf_factory, train, test_sets, matching_indexes, args)\n",
      "File \u001b[0;32m~/Projects/playground/CAL_sim/learner_functions.py:231\u001b[0m, in \u001b[0;36mrun_multiple_experiments\u001b[0;34m(clf_factory, train, test_sets, matching_indexes, args)\u001b[0m\n\u001b[1;32m    229\u001b[0m     experiment_output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39moutdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mframework\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mclass_imbalance\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39minit_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mcold_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mquery_strategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mquery_n\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Set up logging\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/log.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n\u001b[1;32m    232\u001b[0m logging\u001b[38;5;241m.\u001b[39mcaptureWarnings(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m logf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_output_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/err.log\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/logging/__init__.py:2050\u001b[0m, in \u001b[0;36mbasicConfig\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2048\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 2050\u001b[0m     h \u001b[38;5;241m=\u001b[39m FileHandler(filename, mode,\n\u001b[1;32m   2051\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   2052\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2053\u001b[0m     stream \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/logging/__init__.py:1181\u001b[0m, in \u001b[0;36mFileHandler.__init__\u001b[0;34m(self, filename, mode, encoding, delay, errors)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     StreamHandler\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open())\n",
      "File \u001b[0;32m~/anaconda3/envs/sim/lib/python3.11/logging/__init__.py:1213\u001b[0m, in \u001b[0;36mFileHandler._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03mOpen the current base file with the (original) mode and encoding.\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03mReturn the resulting stream.\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m open_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builtin_open\n\u001b[0;32m-> 1213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m open_func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbaseFilename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   1214\u001b[0m                  encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/raymond/Projects/playground/CAL_sim/results/AL_/Users/raymond/Library/Jupyter/runtime/kernel-v2-4635c68Se6l7zTgJ.json_wiki_50_20_BalancedWeak_LeastConfidence()_100/log.txt'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL.model, cache_dir='.cache/')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [\"[URL]\", \"[EMOJI]\", \"[USER]\"]})\n",
    "train_dict = df_to_dict('train', train_df)\n",
    "train = preprocess_data_transformers('train',\n",
    "                                    tokenizer,\n",
    "                                    train_dict['data'],\n",
    "                                    train_dict['target'],\n",
    "                                    train_dict['weak_target'])\n",
    "test_sets = {}\n",
    "matching_indexes = {}\n",
    "for j in test_dfs.keys():\n",
    "    matching_indexes[j] = test_dfs[j].index.tolist()\n",
    "    data_dict = df_to_dict('test', test_dfs[j])\n",
    "    processed_data = preprocess_data_transformers('test',\n",
    "                                                tokenizer,\n",
    "                                                data_dict['data'],\n",
    "                                                data_dict['target'])\n",
    "    test_sets[j] = processed_data\n",
    "# Run experiments\n",
    "run_multiple_experiments(clf_factory, train, test_sets, matching_indexes, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/raymond/Library/Jupyter/runtime/kernel-v2-4635c68Se6l7zTgJ.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_dict['data'], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full = TextDataset(train_encodings, train_dict['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n"
     ]
    }
   ],
   "source": [
    "val_indices = _genenrate_val_indices(train_dict['target'])\n",
    "indices = np.arange(len(train_dict['target']))\n",
    "val_mask = np.isin(indices, val_indices)\n",
    "train_indices = indices[~val_mask]\n",
    "\n",
    "train_dataset = Subset(train_full, train_indices)\n",
    "val_dataset = Subset(train_full, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imbalance: 0.5\n",
      "Setting val indices\n",
      "(20000,)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "indices_neg_label = np.where(train_full.y == 0)[0]\n",
    "indices_pos_label = np.where(train_full.y == 1)[0]\n",
    "all_indices = np.concatenate([indices_neg_label, indices_pos_label])\n",
    "np.random.shuffle(all_indices)\n",
    "x_indices_initial = all_indices.astype(int)\n",
    "y_initial = np.array([train_full.y[i] for i in x_indices_initial])\n",
    "print(f'Starting imbalance: {np.round(np.mean(y_initial),2)}')\n",
    "print('Setting val indices')\n",
    "val_indices = np.concatenate([np.random.choice(indices_pos_label, \n",
    "                                                int(0.1*len(indices_pos_label)),\n",
    "                                                replace=False),\n",
    "                                np.random.choice(indices_neg_label,\n",
    "                                                int(0.1*len(indices_neg_label)),\n",
    "                                                replace=False)\n",
    "                                ])\n",
    "indices = np.arange(x_indices_initial.shape[0])\n",
    "print(indices.shape)\n",
    "mask = np.isin(indices, val_indices)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_full[indices[~mask]]\n",
    "valid = train_full[indices[mask]]\n",
    "train_dataset = TensorDataset(torch.concat(train.x, dim=0), torch.Tensor(train.y))\n",
    "valid_dataset = TensorDataset(torch.concat(valid.x, dim=0), torch.Tensor(valid.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "datasets.logging.set_verbosity_error()\n",
    "\n",
    "# disables the progress bar for notebooks: https://github.com/huggingface/datasets/issues/2651\n",
    "datasets.logging.get_verbosity = lambda: logging.NOTSET\n",
    "\n",
    "# set matplotlib params\n",
    "rcParams.update({'xtick.labelsize': 14, 'ytick.labelsize': 14, 'axes.labelsize': 16})\n",
    "\n",
    "# fix the random seed\n",
    "seed = 2022\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raymond/anaconda3/envs/sim/lib/python3.11/site-packages/small_text/utils/annotations.py:67: ExperimentalWarning: The function from_arrays is experimental and maybe subject to change soon.\n",
      "  warnings.warn(f'The {subject} {func_or_class.__name__} is experimental '\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from small_text import TransformersDataset\n",
    "\n",
    "\n",
    "raw_dataset = datasets.load_dataset('rotten_tomatoes')\n",
    "num_classes = raw_dataset['train'].features['label'].num_classes\n",
    "\n",
    "transformer_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    transformer_model_name\n",
    ")\n",
    "\n",
    "\n",
    "target_labels = np.arange(num_classes)\n",
    "\n",
    "train = TransformersDataset.from_arrays(raw_dataset['train']['text'],\n",
    "                                        raw_dataset['train']['label'],\n",
    "                                        tokenizer,\n",
    "                                        max_length=60,\n",
    "                                        target_labels=target_labels)\n",
    "test = TransformersDataset.from_arrays(raw_dataset['test']['text'], \n",
    "                                       raw_dataset['test']['label'],\n",
    "                                       tokenizer,\n",
    "                                       max_length=60,\n",
    "                                       target_labels=target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2178, 3793, 7099, 1012,  102,    0,    0]], device='mps:0'),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 0, 0]], device='mps:0'),\n",
       " tensor([1], device='mps:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
