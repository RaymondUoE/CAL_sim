{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a6b71e-bdd0-49b4-9362-7a1489fe739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "from contrastive_addition.temp.cleaning_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9646ed77-b4b9-4cbc-8d9b-e8cf880ebbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/raymond/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/raymond/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fc9009e-4979-40ee-bad9-3fee63a0416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wiki(folderpath):\n",
    "    \"\"\"Loads raw wiki data (Wulczyn2017) from folder, cleans text and returns train, test sets.\n",
    "    See https://github.com/ewulczyn/wiki-detox/\n",
    "\n",
    "    Args:\n",
    "        folderpath (str): location of raw dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: train and test sets as pd.Dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'{folderpath}/attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "    annotations = pd.read_csv(f'{folderpath}/attack_annotations.tsv',  sep = '\\t')\n",
    "    # labels a comment as an atack if the majority of annoatators did so\n",
    "    labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
    "    # join binary labels to comments\n",
    "    df['label'] = labels * 1\n",
    "    # remove newline, tab tokens and ==\n",
    "    df['comment'] = df['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "    df['comment'] = df['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "    df['comment'] = df['comment'].apply(lambda x: x.replace(\"==\", \"\"))\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {'comment': 'text'})\n",
    "    # clean data\n",
    "    df = clean_data(df)\n",
    "    # create train, test sets\n",
    "    print('\\n--Creating base test and train sets---')\n",
    "    test = df[df['split']=='test']\n",
    "    train = df[df['split']!='test']\n",
    "    # keep cols\n",
    "    test = test[['clean_text', 'label']]\n",
    "    test = test.rename(columns = {'clean_text':'text'})\n",
    "    abuse = len(test[test['label']==1])\n",
    "    print(f'base_test:\\nlen: {len(test)}, pct_abuse: {np.round(abuse/len(test),3)}')\n",
    "    train = train[['clean_text', 'label']]\n",
    "    train = train.rename(columns = {'clean_text':'text'})\n",
    "    abuse = len(train[train['label']==1])\n",
    "    print(f'base_train:\\nlen: {len(train)}, pct_abuse: {np.round(abuse/len(train),3)}')\n",
    "    return train, test\n",
    "\n",
    "def load_tweets(folderpath):\n",
    "    \"\"\"Loads raw tweets data (Founta2018) from folder, cleans text and returns train, test sets.\n",
    "\n",
    "    Args:\n",
    "        folderpath (str): location of raw dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: train and test sets as pd.Dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(f'{folderpath}/hatespeech_text_label_vote.csv', sep = '\\t',\n",
    "        encoding='utf-8', header=None)\n",
    "    df = df.rename(columns = {0:'tweet', 1:'label', 2:'vote'})\n",
    "    # binarize labels\n",
    "    df['binary_label'] = df['label'].map(lambda x: 0 if x in ['spam', 'normal'] else 1)\n",
    "    # rename columns\n",
    "    df = df.rename(columns = {'tweet':'text'})\n",
    "    # clean data\n",
    "    df = clean_data(df)\n",
    "    # Split of 10% test set\n",
    "    train, test = train_test_split(df, test_size=0.101, shuffle = True, random_state=42)\n",
    "    # create train, test sets\n",
    "    print('\\n--Creating base test and train sets---')\n",
    "    test = test[['clean_text', 'binary_label']]\n",
    "    test = test.rename(columns = {'clean_text':'text', 'binary_label':'label'})\n",
    "    abuse = len(test[test['label']==1])\n",
    "    print(f'base_test:\\nlen: {len(test)}, pct_abuse: {np.round(abuse/len(test),3)}')\n",
    "    train = train[['clean_text', 'binary_label']]\n",
    "    train = train.rename(columns = {'clean_text':'text', 'binary_label':'label'})\n",
    "    abuse = len(train[train['label']==1])\n",
    "    print(f'base_train:\\nlen: {len(train)}, pct_abuse: {np.round(abuse/len(train),3)}')\n",
    "    return train, test\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Cleans data using functions from cleaning_functions.py.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: output cleaned dataframe.\n",
    "    \"\"\"\n",
    "    print('\\n---Dropping NaNs---')\n",
    "    df = drop_nans(df, subset_col = 'text', verbose = True)\n",
    "    print('\\n---Dropping duplicates---')\n",
    "    df = drop_duplicates(df, subset_col = 'text', verbose = True)\n",
    "    print('\\n---Cleaning text---')\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    print('\\n---Dropping empty text entries---')\n",
    "    df = drop_empty_text(df, subset_col = 'clean_text', verbose = True)\n",
    "    print('\\n---Dropping text entries with only URL + EMOJI---')\n",
    "    df = drop_url_emoji(df, subset_col = 'clean_text', verbose = True)\n",
    "    print('\\n---Checking text length---')\n",
    "    df['text_length'] = df['clean_text'].map(lambda x: len(x))\n",
    "    print('Summary statistics of text length:')\n",
    "    print(df['text_length'].describe())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "777cc92d-e96b-496a-ac04-575de8199272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Dropping NaNs---\n",
      "\n",
      "Orig len: 115864,\n",
      "            Num of dropped values: 0,\n",
      "            New len: 115864\n",
      "\n",
      "---Dropping duplicates---\n",
      "\n",
      "Orig len: 115864,\n",
      "            Num of dropped values: 173,\n",
      "            New len: 115691\n",
      "\n",
      "---Cleaning text---\n",
      "\n",
      "---Dropping empty text entries---\n",
      "\n",
      "Orig len: 115691,\n",
      "            Num of dropped values: 4,\n",
      "            New len: 115687\n",
      "\n",
      "---Dropping text entries with only URL + EMOJI---\n",
      "\n",
      "Orig len: 115687,\n",
      "            Num of dropped values: 5,\n",
      "            New len: 115682\n",
      "\n",
      "---Checking text length---\n",
      "Summary statistics of text length:\n",
      "count    115682.000000\n",
      "mean        401.943353\n",
      "std         733.893019\n",
      "min           1.000000\n",
      "25%          89.000000\n",
      "50%         196.000000\n",
      "75%         423.000000\n",
      "max       10000.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "--Creating base test and train sets---\n",
      "base_test:\n",
      "len: 23139, pct_abuse: 0.119\n",
      "base_train:\n",
      "len: 92543, pct_abuse: 0.117\n"
     ]
    }
   ],
   "source": [
    "train, test = load_wiki('./data/Wulczyn2017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e128c61-f9b2-4126-9471-9e63d9f34d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc04208-1cea-4ca5-88dc-7fd1b8b7217d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a889a0-2b86-4bc1-9e5e-fc0897dfcae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7c642-b676-499d-9bfb-43662d238201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d154eb1-2c05-4429-9f0e-7588cb9a26c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
