{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from scipy.special import softmax\n",
    "from collections import Counter\n",
    "import json\n",
    "from keywords_utils import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../../data/wiki'\n",
    "KEYWORDS_PATH = '../../data'\n",
    "OUT_DIR = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{DATA_PATH}/train_original_split.csv')\n",
    "with open('../../../HateXplain/Data/dataset.json', 'r') as f:\n",
    "    exp = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess hatexplain\n",
    "list_of_targets = []\n",
    "sentences = []\n",
    "targets_maj = []\n",
    "\n",
    "for k, v in exp.items():\n",
    "    annotations = v['annotators']\n",
    "    \n",
    "    \n",
    "    all_anno = tuple(x['target'] for x in annotations )\n",
    "    list_of_targets.append(all_anno)\n",
    "    c = Counter([x for y in list(all_anno) for x in y])\n",
    "    ts = [x for x, count in c.items() if count >= 2]\n",
    "    \n",
    "    # removing posts without targets\n",
    "    # if 'None' not in ts:\n",
    "    targets_maj.append(ts)\n",
    "    sentences.append(v['post_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_targets(list_of_targets):\n",
    "    if ('Islam' in list_of_targets or 'mus' in list_of_targets) and ('Jewish' in list_of_targets or 'jew' in list_of_targets):\n",
    "        return 'Both'\n",
    "    elif ('Islam' in list_of_targets or 'mus' in list_of_targets):\n",
    "        return 'Islam'\n",
    "    elif ('Jewish' in list_of_targets or 'jew' in list_of_targets):\n",
    "        return 'Jewish'\n",
    "    else:\n",
    "        return 'Neither'\n",
    "exp_df = pd.DataFrame({'text': [' '.join(x) for x in sentences], 'targets': targets_maj})\n",
    "exp_df['target_reduced'] = exp_df.apply(lambda row: reduce_targets(row['targets']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df.to_csv('preprocessed_explain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess learning from the worst\n",
    "def expand_targets_learning(targets):\n",
    "    splitting_classes = ['asi.wom','bla.wom','non.white.wom','gay.man','asi.man','indig.wom','mus.wom','bla.man','gay.wom']\n",
    "    list_of_targets_flat = []\n",
    "    if targets in splitting_classes:\n",
    "        if len(targets.split('.')) > 2:\n",
    "            list_of_targets_flat += ['.'.join(targets.split('.')[:-1]), targets.split('.')[-1]]\n",
    "        else:\n",
    "            list_of_targets_flat += targets.split('.')\n",
    "    else:\n",
    "        list_of_targets_flat.append(targets)\n",
    "    return list_of_targets_flat\n",
    "\n",
    "    \n",
    "\n",
    "with open('../../../Dynamically-Generated-Hate-Speech-Dataset/Dynamically Generated Hate Dataset v0.2.3.csv', 'r') as f:\n",
    "    learning_df = pd.read_csv(f)\n",
    "\n",
    "learning_df = learning_df[learning_df['split']=='train']\n",
    "learning_df['target_reduced'] = learning_df.apply(lambda row: reduce_targets(expand_targets_learning(row['target'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_df[['text', 'target_reduced']].to_csv('preprocessed_learning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Number of Classes---\n",
      "3\n",
      "---Building Inverse Stemming Lookup---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446403/446403 [00:03<00:00, 111654.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Vocab Size Post Stemming---\n",
      "16585\n",
      "---Vocab Size with minimum frequency 2---\n",
      "11774\n",
      "---Number of Keywords per Class with Threshold 0.7---\n",
      "Neither: 10801\n",
      "Jewish: 28\n",
      "Islam: 12\n",
      "---Writing Keywords to keywords_learning.txt\n",
      "Keys: dict_keys(['Neither', 'Jewish', 'Islam'])\n"
     ]
    }
   ],
   "source": [
    "dataset = 'learning'\n",
    "train = pd.read_csv(f'./preprocessed_{dataset}.csv')\n",
    "keywords, inverse_stem_lookup= gen_keywords(train, p=0.7, tail_cutoff=2, col='target_reduced', txt='text')\n",
    "\n",
    "out_dict = {}\n",
    "for k, v in keywords.items():\n",
    "    pre_stemmed = []\n",
    "    for w in v:\n",
    "        pre_stemmed += inverse_stem_lookup[w]\n",
    "    out_dict[k] = pre_stemmed\n",
    "\n",
    "print(f'---Writing Keywords to keywords_{dataset}.txt')\n",
    "print(f'Keys: {out_dict.keys()}')\n",
    "with open(f'./keywords_{dataset}.txt', 'w') as f:\n",
    "    json.dump(out_dict, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>acl.id</th>\n",
       "      <th>X1</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>type</th>\n",
       "      <th>target</th>\n",
       "      <th>level</th>\n",
       "      <th>split</th>\n",
       "      <th>round.base</th>\n",
       "      <th>annotator</th>\n",
       "      <th>round</th>\n",
       "      <th>acl.id.matched</th>\n",
       "      <th>target_reduced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>1329</td>\n",
       "      <td>acl11193</td>\n",
       "      <td>1329</td>\n",
       "      <td>The smell of jews is digusting, it's like a pi...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>2a</td>\n",
       "      <td>acl11194</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>1335</td>\n",
       "      <td>acl11199</td>\n",
       "      <td>1335</td>\n",
       "      <td>idk about you but I believe jews are controlli...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>2a</td>\n",
       "      <td>acl11200</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1359</th>\n",
       "      <td>1394</td>\n",
       "      <td>acl11251</td>\n",
       "      <td>1394</td>\n",
       "      <td>There's just something about kikes that I don'...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>2a</td>\n",
       "      <td>acl11252</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>1487</td>\n",
       "      <td>acl11335</td>\n",
       "      <td>1487</td>\n",
       "      <td>Not buying from nasty kike-owned stores again....</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>perturbation</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>CAgNlUizNm</td>\n",
       "      <td>2b</td>\n",
       "      <td>acl20643</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>1498</td>\n",
       "      <td>acl11345</td>\n",
       "      <td>1498</td>\n",
       "      <td>THIS IS THE REASON I WAKE UP THE THE MORNING F...</td>\n",
       "      <td>hate</td>\n",
       "      <td>support</td>\n",
       "      <td>jew</td>\n",
       "      <td>perturbation</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>2b</td>\n",
       "      <td>acl11344</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31567</th>\n",
       "      <td>31651</td>\n",
       "      <td>acl38484</td>\n",
       "      <td>31651</td>\n",
       "      <td>WITNESS what has been done ! their lives are w...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>perturbation</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>LqLKTtrOmx</td>\n",
       "      <td>4b</td>\n",
       "      <td>acl39738</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31570</th>\n",
       "      <td>31654</td>\n",
       "      <td>acl38487</td>\n",
       "      <td>31654</td>\n",
       "      <td>#blind, for not seeing how this #pandemic is o...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>perturbation</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>GNZuCtwed3</td>\n",
       "      <td>4b</td>\n",
       "      <td>acl39741</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31635</th>\n",
       "      <td>31719</td>\n",
       "      <td>acl38545</td>\n",
       "      <td>31719</td>\n",
       "      <td>Russian tourist offered employee $1 million to...</td>\n",
       "      <td>hate</td>\n",
       "      <td>animosity</td>\n",
       "      <td>jew</td>\n",
       "      <td>perturbation</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>cYKSVBW4HD</td>\n",
       "      <td>4b</td>\n",
       "      <td>acl39791</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31809</th>\n",
       "      <td>31893</td>\n",
       "      <td>acl38701</td>\n",
       "      <td>31893</td>\n",
       "      <td>i do lmao, me and him share the same \"opinion\"...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>4a</td>\n",
       "      <td>acl38702</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31889</th>\n",
       "      <td>31973</td>\n",
       "      <td>acl38774</td>\n",
       "      <td>31973</td>\n",
       "      <td>imagine if Hitler gave up? All of western euro...</td>\n",
       "      <td>hate</td>\n",
       "      <td>derogation</td>\n",
       "      <td>jew</td>\n",
       "      <td>original</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>QiOKkCi7F8</td>\n",
       "      <td>4a</td>\n",
       "      <td>acl38775</td>\n",
       "      <td>Jewish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    acl.id     X1  \\\n",
       "1294         1329  acl11193   1329   \n",
       "1300         1335  acl11199   1335   \n",
       "1359         1394  acl11251   1394   \n",
       "1452         1487  acl11335   1487   \n",
       "1463         1498  acl11345   1498   \n",
       "...           ...       ...    ...   \n",
       "31567       31651  acl38484  31651   \n",
       "31570       31654  acl38487  31654   \n",
       "31635       31719  acl38545  31719   \n",
       "31809       31893  acl38701  31893   \n",
       "31889       31973  acl38774  31973   \n",
       "\n",
       "                                                    text label        type  \\\n",
       "1294   The smell of jews is digusting, it's like a pi...  hate  derogation   \n",
       "1300   idk about you but I believe jews are controlli...  hate  derogation   \n",
       "1359   There's just something about kikes that I don'...  hate  derogation   \n",
       "1452   Not buying from nasty kike-owned stores again....  hate  derogation   \n",
       "1463   THIS IS THE REASON I WAKE UP THE THE MORNING F...  hate     support   \n",
       "...                                                  ...   ...         ...   \n",
       "31567  WITNESS what has been done ! their lives are w...  hate  derogation   \n",
       "31570  #blind, for not seeing how this #pandemic is o...  hate  derogation   \n",
       "31635  Russian tourist offered employee $1 million to...  hate   animosity   \n",
       "31809  i do lmao, me and him share the same \"opinion\"...  hate  derogation   \n",
       "31889  imagine if Hitler gave up? All of western euro...  hate  derogation   \n",
       "\n",
       "      target         level  split  round.base   annotator round  \\\n",
       "1294     jew      original  train           2  CAgNlUizNm    2a   \n",
       "1300     jew      original  train           2  CAgNlUizNm    2a   \n",
       "1359     jew      original  train           2  CAgNlUizNm    2a   \n",
       "1452     jew  perturbation  train           2  CAgNlUizNm    2b   \n",
       "1463     jew  perturbation  train           2  QiOKkCi7F8    2b   \n",
       "...      ...           ...    ...         ...         ...   ...   \n",
       "31567    jew  perturbation  train           4  LqLKTtrOmx    4b   \n",
       "31570    jew  perturbation  train           4  GNZuCtwed3    4b   \n",
       "31635    jew  perturbation  train           4  cYKSVBW4HD    4b   \n",
       "31809    jew      original  train           4  QiOKkCi7F8    4a   \n",
       "31889    jew      original  train           4  QiOKkCi7F8    4a   \n",
       "\n",
       "      acl.id.matched target_reduced  \n",
       "1294        acl11194         Jewish  \n",
       "1300        acl11200         Jewish  \n",
       "1359        acl11252         Jewish  \n",
       "1452        acl20643         Jewish  \n",
       "1463        acl11344         Jewish  \n",
       "...              ...            ...  \n",
       "31567       acl39738         Jewish  \n",
       "31570       acl39741         Jewish  \n",
       "31635       acl39791         Jewish  \n",
       "31809       acl38702         Jewish  \n",
       "31889       acl38775         Jewish  \n",
       "\n",
       "[891 rows x 14 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_df[learning_df['target']=='jew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokens_full</th>\n",
       "      <th>tokens_clean</th>\n",
       "      <th>tokens_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>:If I may butt in I've spent the last 1/4 hour...</td>\n",
       "      <td>0</td>\n",
       "      <td>[:, If, I, may, butt, in, I, 've, spent, the, ...</td>\n",
       "      <td>[if, i, may, butt, i, spent, last, hour, follo...</td>\n",
       "      <td>[if, i, may, butt, i, spent, last, hour, follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>On my you will find the apology that I owe you...</td>\n",
       "      <td>0</td>\n",
       "      <td>[On, my, you, will, find, the, apology, that, ...</td>\n",
       "      <td>[on, find, apology, i, owe, shuffles, feet, lo...</td>\n",
       "      <td>[on, find, apolog, i, owe, shuffl, feet, look,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yep, that's Twin cities from which this articl...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Yep, ,, that, 's, Twin, cities, from, which, ...</td>\n",
       "      <td>[yep, twin, cities, article, originated]</td>\n",
       "      <td>[yep, twin, citi, articl, origin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>See? I was right! ;-)</td>\n",
       "      <td>0</td>\n",
       "      <td>[See, ?, I, was, right, !, ;, -, )]</td>\n",
       "      <td>[see, i, right]</td>\n",
       "      <td>[see, i, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>` Thanks for fixing the spelling error in ``pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[`, Thanks, for, fixing, the, spelling, error,...</td>\n",
       "      <td>[thanks, fixing, spelling, error, propaganda, ...</td>\n",
       "      <td>[thank, fix, spell, error, propaganda, i, ca, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23134</th>\n",
       "      <td>` kys {| style=``background-color: #fdffe7; bo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[`, kys, {, |, style=, ``, background-color, :...</td>\n",
       "      <td>[kys, fdffe7, border, 1px, solid, fceb92, 2, m...</td>\n",
       "      <td>[ky, fdffe7, border, 1px, solid, fceb92, 2, mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23135</th>\n",
       "      <td>Yeah, I realized I created a duplicate ID. Sor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Yeah, ,, I, realized, I, created, a, duplicat...</td>\n",
       "      <td>[yeah, i, realized, i, created, duplicate, id,...</td>\n",
       "      <td>[yeah, i, realiz, i, creat, duplic, id, sorri,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23136</th>\n",
       "      <td>` :::Yeah and in the earlier sentence I'd rewo...</td>\n",
       "      <td>0</td>\n",
       "      <td>[`, :, :, :, Yeah, and, in, the, earlier, sent...</td>\n",
       "      <td>[yeah, earlier, sentence, i, reword, fever, pi...</td>\n",
       "      <td>[yeah, earlier, sentenc, i, reword, fever, pitch]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23137</th>\n",
       "      <td>Why oh why... You removed the trolls ANI secti...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Why, oh, why, ..., You, removed, the, trolls,...</td>\n",
       "      <td>[why, oh, you, removed, trolls, ani, section, ...</td>\n",
       "      <td>[whi, oh, you, remov, troll, ani, section, drm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23138</th>\n",
       "      <td>The Institute for Historical Review is a peer-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[The, Institute, for, Historical, Review, is, ...</td>\n",
       "      <td>[the, institute, historical, review, journal, ...</td>\n",
       "      <td>[the, institut, histor, review, journal, well,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23139 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      :If I may butt in I've spent the last 1/4 hour...      0   \n",
       "1      On my you will find the apology that I owe you...      0   \n",
       "2      Yep, that's Twin cities from which this articl...      0   \n",
       "3                                  See? I was right! ;-)      0   \n",
       "4      ` Thanks for fixing the spelling error in ``pr...      0   \n",
       "...                                                  ...    ...   \n",
       "23134  ` kys {| style=``background-color: #fdffe7; bo...      0   \n",
       "23135  Yeah, I realized I created a duplicate ID. Sor...      0   \n",
       "23136  ` :::Yeah and in the earlier sentence I'd rewo...      0   \n",
       "23137  Why oh why... You removed the trolls ANI secti...      0   \n",
       "23138  The Institute for Historical Review is a peer-...      0   \n",
       "\n",
       "                                             tokens_full  \\\n",
       "0      [:, If, I, may, butt, in, I, 've, spent, the, ...   \n",
       "1      [On, my, you, will, find, the, apology, that, ...   \n",
       "2      [Yep, ,, that, 's, Twin, cities, from, which, ...   \n",
       "3                    [See, ?, I, was, right, !, ;, -, )]   \n",
       "4      [`, Thanks, for, fixing, the, spelling, error,...   \n",
       "...                                                  ...   \n",
       "23134  [`, kys, {, |, style=, ``, background-color, :...   \n",
       "23135  [Yeah, ,, I, realized, I, created, a, duplicat...   \n",
       "23136  [`, :, :, :, Yeah, and, in, the, earlier, sent...   \n",
       "23137  [Why, oh, why, ..., You, removed, the, trolls,...   \n",
       "23138  [The, Institute, for, Historical, Review, is, ...   \n",
       "\n",
       "                                            tokens_clean  \\\n",
       "0      [if, i, may, butt, i, spent, last, hour, follo...   \n",
       "1      [on, find, apology, i, owe, shuffles, feet, lo...   \n",
       "2               [yep, twin, cities, article, originated]   \n",
       "3                                        [see, i, right]   \n",
       "4      [thanks, fixing, spelling, error, propaganda, ...   \n",
       "...                                                  ...   \n",
       "23134  [kys, fdffe7, border, 1px, solid, fceb92, 2, m...   \n",
       "23135  [yeah, i, realized, i, created, duplicate, id,...   \n",
       "23136  [yeah, earlier, sentence, i, reword, fever, pi...   \n",
       "23137  [why, oh, you, removed, trolls, ani, section, ...   \n",
       "23138  [the, institute, historical, review, journal, ...   \n",
       "\n",
       "                                             tokens_stem  \n",
       "0      [if, i, may, butt, i, spent, last, hour, follo...  \n",
       "1      [on, find, apolog, i, owe, shuffl, feet, look,...  \n",
       "2                      [yep, twin, citi, articl, origin]  \n",
       "3                                        [see, i, right]  \n",
       "4      [thank, fix, spell, error, propaganda, i, ca, ...  \n",
       "...                                                  ...  \n",
       "23134  [ky, fdffe7, border, 1px, solid, fceb92, 2, mi...  \n",
       "23135  [yeah, i, realiz, i, creat, duplic, id, sorri,...  \n",
       "23136  [yeah, earlier, sentenc, i, reword, fever, pitch]  \n",
       "23137  [whi, oh, you, remov, troll, ani, section, drm...  \n",
       "23138  [the, institut, histor, review, journal, well,...  \n",
       "\n",
       "[23139 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens_full'] = df.apply(lambda x: word_tokenize(x[txt]), axis=1)\n",
    "df['tokens_clean'] = df.apply(lambda x: [w.lower() for w in x['tokens_full'] if w not in stop_words and w.isalnum()], axis=1)\n",
    "df['tokens_stem'] = df.apply(lambda x: [ps.stem(w) for w in x['tokens_clean']], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [x for y in df['tokens_stem'] for x in y]\n",
    "count = Counter(tokens)\n",
    "count_class = Counter(df['label'])\n",
    "\n",
    "VOCAB_SIZE = len(count.keys())\n",
    "NUM_OF_TOKENS = sum(count.values())\n",
    "NUM_OF_CLASSES = len(count_class.keys())\n",
    "NUM_OF_RECORDS = sum(count_class.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40553"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "for k, v in count.items():\n",
    "    if v == 1:\n",
    "        c +=1\n",
    "        \n",
    "len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23139"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(count_class.values())\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = np.zeros((VOCAB_SIZE, NUM_OF_CLASSES))\n",
    "prior = np.zeros((NUM_OF_CLASSES,1))\n",
    "norm = np.zeros((VOCAB_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(count.keys())\n",
    "classes = list(count_class.keys())\n",
    "for k, v in count_class.items():\n",
    "    sub_df = df[df['label']==k]\n",
    "    sub_tokens = [x for y in sub_df['tokens_stem'] for x in y]\n",
    "    prior[classes.index(k), 0] = v / len(df)\n",
    "    sub_token_count = Counter(sub_tokens)\n",
    "    for s_token, s_count in sub_token_count.items():\n",
    "        likelihood[vocab.index(s_token), classes.index(k)] = s_count / sum(sub_token_count.values())\n",
    "        \n",
    "for k, v in count.items():\n",
    "    norm[vocab.index(k), 0] = v / NUM_OF_TOKENS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90466626, 0.09664234],\n",
       "       [0.8807418 , 0.11923531],\n",
       "       [0.98000006, 0.02550114],\n",
       "       ...,\n",
       "       [1.00700407, 0.        ],\n",
       "       [0.        , 0.95096063],\n",
       "       [1.00700407, 0.        ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood * np.repeat(prior,VOCAB_SIZE,axis=1).T / np.repeat(norm, NUM_OF_CLASSES, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "915554"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_OF_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
